---
title: 集成学习
tags:
  - 机器学习
cover: https://pic.imgdb.cn/item/649ce61e1ddac507ccb10aca.png
toc: True
date: 2023/7/28
---

## 文章结构

<img src="https://pic.imgdb.cn/item/64c3c6121ddac507ccc450ec.jpg" style="zoom:67%;" />

## 1 集成学习算法

1.1 什么是集成学习

​		集成学习是将多个弱机器学习器结合，构建一个有较强性能的机器学习器的方法，也就是通过构建并合并多个学习器来完成学习任务，其中构成集成学习的弱学习器称为**基学习器、基估计器**。

- 根据集成学习的各基估计器类型是否相同，可以分为：同质和异质。

  - 同质：指个体学习器全是同一类型。

  - 异质：指个体学习器包含不同类型的学习算法。

- 根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即
  个体学习器间**存在强依赖关系、必须串行生成**的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法。前者的代表是**Boosting**，后者的代表是Bagging和“随机森林”(Random Forest)。

<img src="https://pic.imgdb.cn/item/64c38cfa1ddac507cc5b9cad.jpg" style="zoom:67%;" />

## 2 集成学习常用方法

**2.1 Boosting**

​		Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似于：先从初始训练集训练出一个基学习器,再根据基学习器的表现对训练样本分布进行调整,使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

​		代表算法：**Adaboost**，GBDT，XGBoost，LightGBM

**2.1.1 实现过程**

1. 训练第一个学习器

<img src="https://pic.imgdb.cn/item/64c38e0a1ddac507cc5d60a3.jpg" style="zoom: 50%;" />

2. 调整数据分布

<img src="https://pic.imgdb.cn/item/64c38e1b1ddac507cc5d7ab8.jpg" style="zoom: 50%;" />

3. 训练第二个学习器

<img src="https://pic.imgdb.cn/item/64c38e4a1ddac507cc5dc906.jpg" style="zoom: 50%;" />

4. 再次调整数据分布

<img src="https://pic.imgdb.cn/item/64c38e811ddac507cc5e2124.jpg" style="zoom:50%;" />

5. 依次训练学习器，调整数据分布

<img src="https://pic.imgdb.cn/item/64c38ea11ddac507cc5e5311.jpg" style="zoom:50%;" />

**2.2 AdaBoost介绍**

**2.2.1 构造细节**

- 步骤一：初始化训练数据权重相等，训练第一个学习器。

  - 该假设每个训练样本在基分类器的学习中作用相同，这一假设可以保证第一步能够在原始数据上学习基本分类器$H_1(x)$

- 步骤二：AdaBoost反复学习基本分类器，在每一轮$m= 1, 2, ... M$顺次的执行下列操作:
  
   - 在权值分布为$D_t$的训练数据上，确定基分类器；
   
   - 计算该学习器在训练数据中的错误率：$ε_t=P(h_t(x_t)\neq y_t)$
   
     > 错误率=0.5，没有投票权重
     > 错误率>0.5，投负权重票
     > 错误率<0.5，投正权重票
   
   - 计算该学习器的投票权重：$α_t=\frac{1}{2}ln(\frac{1-ε_t}{ε_t})$
   
   - 根据投票权重，对训练数据重新赋权：$D_{t+1}(x)=\frac{D_t(x)}{Z_t}*\left\{\begin{matrix}e^{-α_t},预测值=真实值\\e^{α_t},预测值\neq 真实值\end{matrix}\right.$
   
     >  $Z_t$为归一化系数，将权重缩放到0-1
   
   - 重复执行以上4步，m次。
   
- 步骤三：对m个学习器进行加权投票

   $$H(x)=sign(\sum\limits_{i=1}^{m}α_ih_i(x))$$

   > 当数字为正数时，则返回1
   >
   > 当数字为0时，则返回0
   >
   > 当数字为负数时，则返回-1

**2.2.2 如何确认投票权重**？

​		核心原则：正确率越高，投票权中越大。”乱猜“的投票权重为0。

**2.2.2 如何调整数据分布**？

​		核心原则：前一轮学习正确的数据，忽略；前一轮学习错误的数据，重视。

**2.2.3 案例分析**

​		给定下面这张训练数据表所示的数据，假设弱分类器由xv产生，其阈值v使该分类器在训练数据集上的分类误差率最低，试用Adaboost算法学习一个强分类器。

|  x   |  0   |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  y   |  1   |  1   |  1   |  -1  |  -1  |  -1  |  1   |  1   |  1   |  -1  |

- 步骤一：初始化训练数据权重相等，训练第一个学习器。

  > $D_1=(w_{11},w_{12},...,w_{110})$
  >
  > $w_{1i}=0.1,i=1,2,...,10$

- AdaBoost反复学习基本分类器，在每一轮$m= 1, 2, ... M$顺次的执行下列操作：

  > **当m=1时：**
  >
  > - 在权重分布为$D_1$的训练数据上，阈值v取2.5时分类误差率最低，故基本分类器为$h_1(x)=\left\{\begin{matrix}1,x<2.5\\-1,x>2.5\end{matrix}\right.$
  >
  > - 计算该学习器在训练数据中的错误率（6，7，8错误）：$ε_1=P(h_1(x_1)\neq y_1)=0.1*3=0.3$
  >
  > - 计算该学习器的投票权重：$α_1=\frac{1}{2}ln(\frac{1-ε_1}{ε_1})=0.4236$
  >
  > - 根据投票权重，对训练数据重新赋权：
  >
  >   $$\frac{0.1}{e^{0.4236}*0.3+e^{-0.4236}*0.7}*e^{0.4236}=0.16667$$
  >
  >   $$D_2=(0.07143, 0.07143, 0.07143, 0.07143, 0.07143, 0.07143, 0.16667, 0.16667, 0.16667, 0.07143)$$
  >
  >   $$H_1(x)=sign[0.4236*h_1(x)]$$ 
  >
  >   分类器$H_1(x)$在训练数据集上有3个误分类点。
  >
  > **当m=2时：**
  >
  > > 阈值v取2.5时，误差率为0.16667 * 3（x < 2.5时取1，x > 2.5时取-1，则6 7 8分错，误差率为0.16667*3）
  > >
  > > 阈值v取5.5时，误差率最低为0.07143 * 4（x > 5.5时取1，x < 5.5时取-1，则0 1 2 9分错，误差率为0.07143 *4）
  > >
  > > **阈值v取8.5时，误差率为0.07143 * 3（x < 8.5时取1，x > 8.5时取-1，则3 4 5分错，误差率为0.07143*3）**
  >
  > - 在权重分布为$D_2$的训练数据上，阈值v取8.5时，分类误差率最低，故基本分类器为$h_2(x)=\left\{\begin{matrix}1,x<8.5\\-1,x>8.5\end{matrix}\right.$
  >
  > - 计算该学习器在训练数据中的错误率（3，4，5错误）：$ε_2=P(h_2(x_2)\neq y_2)=0.07143*3=0.2143$
  >
  > - 计算该学习器的投票权重：$α_2=\frac{1}{2}ln(\frac{1-ε_2}{ε_2})=0.6496$
  >
  > - 根据投票权重，对训练数据重新赋权：
  >
  >   $$D_3= (0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.1667, 0.1060, 0.1060, 0.1060, 0.0455)$$
  >
  >   $$H_2(x)=sign[0.4236*h_1(x)+0.6496*h_2(x)]$$ 
  >
  >   分类器$H_2(x)$在训练数据集上有3个误分类点。
  >
  > **当m=3时：**
  >
  > > 阈值v取2.5时，误差率为0.3180（x < 2.5时取1，x > 2.5时取-1，则6 7 8分错，误差率为0.1060\*3）
  > > **阈值v取5.5时，误差率最低为0.1820（x > 5.5时取1，x < 5.5时取-1，则0 1 2 9分错，误差率为0.0455*4= 0.1820）**
  > > 阈值v取8.5时，误差率为0.5001（x < 8.5时取1，x > 8.5时取-1，则3 4 5分错，误差率为0.1667\*3）
  >
  > - 在权重分布为$D_3$的训练数据上，阈值v取5.5时分类误差率最低，故基本分类器为$h_3(x)=\left\{\begin{matrix}1,x<5.5\\-1,x>5.5\end{matrix}\right.$
  >
  > - 计算该学习器在训练数据中的错误率（6，7，8错误）：$ε_3=P(h_3(x_3)\neq y_3)=0.0455*4=0.1820$
  >
  > - 计算该学习器的投票权重：$α_3=\frac{1}{2}ln(\frac{1-ε_3}{ε_3})=0.7514$
  >
  > - 根据投票权重，对训练数据重新赋权：
  >
  >   $$D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)$$
  >
  >   $$H_3(x)=sign[0.4236*h_1(x)+0.6496*h_2(x)+0.7514*h_3(x)]$$ 
  >
  >   分类器$H_3(x)$在训练数据集上有0个误分类点。
  >
  >   > 权重求和为正数，映射为+1
  >   > 权重求和为负数，映射为-1

|       |         | 2.5  |         | 5.5  |         | 8.5  |         |
| :---: | :-----: | :--: | :-----: | :--: | :-----: | :--: | :-----: |
| $h_1$ | 0.4236  |  -   | -0.4236 |  -   | -0.4236 |  -   | -0.4236 |
| $h_2$ | 0.6496  |  -   | 0.6496  |  -   | 0.6496  |  -   | -0.6496 |
| $h_3$ | -0.7514 |  -   | -0.7514 |  -   | 0.7514  |  -   | 0.7514  |
|       |   +1    |      |   -1    |      |   +1    |      |   -1    |

​		从上述过程中可以发现，如果某些个样本被分错，它们在下一轮迭代中的权值将被增大，同时，其它被分对的样本在下一轮迭代中的权值将被减小。就这样，分错样本权值增大，分对样本权值变小，而在下一轮迭代中，总是选取让误差率最低的阈值来设计基本分类器，所以误差率不断降低。

**2.3 Bagging**

- 用抽样的方式从原始样本中进行有放回的多次抽样（或者是抽特征），这种方法叫做Bootstraping，抽取k次每次抽取n个样本，这样就生成了k个样本容量为n的数据集。原始数据集中的样本可能是多次被抽到也可能是没有被抽到。
- 每次使用一个数据即选练得到一个模型，这样k个数据集就可以得到k个模型。（模型可以是根据问题来选择，回归或者分类模型都可以）这里就得到了k个弱分类器。
- 对新的数据使用所有的模型进行分类，然后采用voting的方式来确定结果。统计所有模型的计算结果，然后选出票数最高的那个作为结果。（如果是回归问题则以所有模型的均值作为最终结果。）

**2.3.1 实现过程**

<img src="https://pic.imgdb.cn/item/64c3c27b1ddac507ccbd171b.jpg" style="zoom:67%;" />



**2.4 bagging集成与boosting集成的区别**

|   区别   |                     bagging集成                      |                     boosting集成                      |
| :------: | :--------------------------------------------------: | :---------------------------------------------------: |
| 数据方面 |                  对数据进行采样训练                  |          根据前一轮学习结果调整数据的重要性           |
| 投票方面 |                  所有学习器平权投票                  |                 对学习器进行加权投票                  |
| 学习顺序 |    Bagging的学习是并行的，每个学习器没有依赖关系     |          Boosting学习是串行，学习有先后顺序           |
| 主要作用 | 主要用于提高泛化性能（解决过拟合，也可以说降低方差） | 主要用于提高训练精度 （解决欠拟合，也可以说降低偏差） |

**2.5 随机森林**

**2.5.1 构造过程**

​		在机器学习中，**随机森林是一个包含多个决策树的分类器**，并且其输出的类别是由个别树输出的类别的众数而定。

> **随机森林 = Bagging + 决策树**

<img src="https://pic.imgdb.cn/item/64c3c2e51ddac507ccbde502.jpg" style="zoom:67%;" />

**随机森林构造过程中的关键步骤**(M表示特征数目)：

- 一次随机选出一个样本，有放回的抽样，重复N次(有可能出现重复的样本)

- 随机去选出m个特征, m<<M，建立决策树

> 为什么要随机抽样训练集？
>
> ​		**如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的**
>
> 为什么要有放回地抽样？
>
> ​		如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是“有偏的”，都是绝对“片面的”，也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树(弱分类器)的投票表决。

## 3 结合策略

**3.1 平均法**

1. 简单平均法

   $$H(x)=\frac{1}{T}\sum\limits_{i=1}^{T}h_i(x)$$

2. 加权平均法

   $$H(x)=\sum\limits_{i=1}^{T}w_i(x)h_i(x)$$

   > 其中$w_i$是个体学习器$h_i$的权重，通常要求$w_i≥0$,$\sum\limits_{i=1}^{T}w_i=1$
   >
   > 一般而言,在个体学习器性能相差较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。

**3.2 投票法**

1. 绝对多数投票法

​	<img src="https://pic.imgdb.cn/item/64c3c7fc1ddac507ccc8065c.jpg" style="zoom:67%;" />

   > 即若某标记得票过半数,则预测为该标记;否则拒绝预测.

2. 相对多数投票法

​		$$H(x)=c_{\arg \max \limits_{j}\sum \limits_{i=1}^{T}h_i^j(x)}$$

> 即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个。

3. 加权投票法

​			$$H(x)=c_{\arg \max \limits_{j}\sum \limits_{i=1}^{T}w_ih_i^j(x)}$$

> 其中$w_i$是个体学习器$h_i$的权重，通常要求$w_i≥0$,$\sum\limits_{i=1}^{T}w_i=1$

**3.3 学习法**

**初级学习器**：个体学习器
**次级学习器（元学习器）**：用于结合的学习器

**3.3.1 Stacking算法**
		先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器，**在这个新数据集中，初级学习器的输出被当作样例输入特征，而初始样本的标记仍被当作样例标记**。假定初级学习器使用不同学习算法产生（即初级集成是**异质的**)。

<img src="https://pic.imgdb.cn/item/64c3caf41ddac507ccce1033.jpg" style="zoom:67%;" />

## 4 多样性

**4.1 误差-分歧分解**

<img src="https://pic.imgdb.cn/item/64c3cb3c1ddac507ccceaf68.jpg" style="zoom:67%;" />

**4.2 多样性度量**

​		顾名思义，多样性度量(diversity measure)是用于度量集成中个体分类器的多样性，即估算个体学习器的多样化程度。典型做法是考虑个体分类器的两两相似/不相似性。

<img src="https://pic.imgdb.cn/item/64c3cc251ddac507ccd089eb.jpg" style="zoom:67%;" />

**4.3 多样性增强**

常见的增强个体学习器多样性的方法：数据样本扰动、输入属性扰动、输出表示扰动、算法参数扰动。

1. 数据样本扰动：通常基于采样法(如Bagging中的自助采样、Adaboost中的序列采样)对“不稳定基学习器”很有效；

2. 输入属性扰动：不同的“子空间”(即属性子集)提供了观察数据的不同视角，从不同子空间训练的个体学习集必然有所不同。算法从初始属性集中抽取出若干个属性子集，再基于每个属性子集训练一个基学习器。

3. 输出表示扰动：对输出表示进行操作以增强多样性。可对训练个样本的类标记稍作变动，如翻转法（flipping output）随机改变一些训练样本的标记等等。

4. 算法参数扰动：通过随机设置不同参数，可产生差异较大的个体学习器。

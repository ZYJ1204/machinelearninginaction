---
title: 树回归
tags:
  - 机器学习
cover: https://pic.imgdb.cn/item/649ce61e1ddac507ccb10aca.png
toc: True
date: 2023/8/9
---

# 树回归

### 本章内容

- CART算法
- 回归与模型树
- 树剪枝算法

---

### 1 CART回归树

**1.1 回归原理**

​		CART回归树是二叉树，它的损失函数是最小均方差(MSE)：
$$
L=-\frac{1}{2}\cdot\sum_{i=1}^{N}(y_{i}-\hat{y})^2
$$
​		其中，N是样本数，$\hat{y}$是我们的估计值。我们把损失函数L对$\hat{y}$求导并令其为0：
$$
\frac{\partial{L}}{\partial\hat{y}}=\sum_{i=1}^{N}(y_{i}-\dot{y})=\sum_{i=1}^{N}y_{i}-N\cdot\dot{y}=0
$$
​		解得：
$$
\hat{y}=\frac{1}{N}\cdot\sum_{i=1}^Ny_i
$$
​		因此我们要用这个算法预测一个值，取y的平均值是一个最佳的选择。

​		既然CART是一个二叉树做回归，那么它会在切分点s出把数据集分为两部分，并根据MSE作为损失函数使之最小，其数学描述是：
$$
\min\limits_{j,s}[\min\limits_{c_1}\sum_{x_i∈R_1(j,s)}(y_i-c_1)^2+\min\limits_{c_2}\sum_{x_i∈R_2(j,s)}(y_i-c_2)^2]
$$
​		上式中，j是最优切分变量，s是切分点。

​		由公式看出，第一个切分点把数据分成了两部分，而$R_1$数据集又会被一个切分点分成两部分，$R_2	$也是如此，直至满足条件才会停止切分。$c_1,c_2$就是每个被切分的数据集的最优估计值S。

**1.2 计算流程**

|  x   |  1   |  2   |  3   |  4   |  5   |  6   |  7   |  8   |  9   |  10  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  y   | 5.56 | 5.7  | 5.91 | 6.4  | 6.8  | 7.05 | 8.9  | 8.7  |  9   | 9.05 |

1. 选择最优切分变量j与最优切分点s：

​		 在本数据集中，只有一个变量，因此最优切分变量自然是x。接下来我们考虑9个切分$[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5]$。

2. 根据公式，来对数据集进行切分：

​		首先尝试计算第一个切分点 s=1.5，s=1.5把数据集分成了两部分：

|  x   |  1   |
| :--: | :--: |
|  y   | 5.56 |

|  x   |  2   |  3   |  4   |                              5                               |  6   |  7   |  8   |  9   |  10  |
| :--: | :--: | :--: | :--: | :----------------------------------------------------------: | :--: | :--: | :--: | :--: | :--: |
|  y   | 5.7  | 5.91 | 6.4  | xxxxxxxxxx16 1frozenset({2, 3})2[frozenset({2}), frozenset({3})]3frozenset({3, 5})4[frozenset({3}), frozenset({5})]5frozenset({2, 5})6[frozenset({2}), frozenset({5})]7Rule： frozenset({5}) --> frozenset({2}) confidence: 1.08Rule： frozenset({2}) --> frozenset({5}) confidence: 1.09frozenset({1, 3})10[frozenset({1}), frozenset({3})]11Rule： frozenset({1}) --> frozenset({3}) confidence: 1.012frozenset({2, 3, 5})13[frozenset({2}), frozenset({3}), frozenset({5})]14[(frozenset({5}), frozenset({2}), 1.0),15 (frozenset({2}), frozenset({5}), 1.0),16 (frozenset({1}), frozenset({3}), 1.0)]python | 7.05 | 8.9  | 8.7  |  9   | 9.05 |

​		两部分的最优估计值(平均值)： $c_1=5.56,c_2=(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05)/9=7.5$

​		同理，计算切分点s=2.5时的$c_1,c_2，s=3.5,...,9.5$时的$c_1,c_2$ 。汇总成表格：

|   s   | 1.5  | 2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  | 8.5  | 9.5  |
| :---: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| $c_1$ | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88 | 7.11 |
| $c_2$ | 7.5  | 7.73 | 7.99 | 8.25 | 8.54 | 8.91 | 8.92 | 9.03 | 9.05 |

​		现在我们计算损失函数的值，比如在切分点s=1.5处：
$$
L_{s=1.5}=(5.56−5.56)^2+[(5.7−7.5)^2+(5.91−7.5)^2+(6.4−7.5)^2+(6.8−7.5)^2+(7.05−7.5)^2+(8.9−7.5)^2+(8.7−7.5)^2+(9−7.5)^2+(9.05−7.5)^2]=0+15.72=15.72
$$
​		同样计算出切分点在s = 2.5,...,9.5时的损失函数，汇总成表格是：

|  s   |  1.5  |  2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  |  8.5  |  9.5  |
| :--: | :---: | :---: | :--: | :--: | :--: | :--: | :--: | :---: | :---: |
| L(s) | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |

​		由此我们可以看出，在切分点s=6.5时，损失函数最小，因此s=6.5是最佳切分点，它把数据集切分成两部分：$R_{1}(j,s)=x|x^{(j)}\leq 6.5$和$R_{2}(j,s)=x|x^{(j)} ＞ 6.5$

3. 然后我们再对这两个子区域继续上述的切分

|  x   |  1   |  2   |  3   |  4   |  5   |  6   |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
|  y   | 5.56 | 5.7  | 5.91 | 6.4  | 6.8  | 7.05 |

 		计算出每个切分点s对应的损失函数值：

|  s   |  1.5   |  2.5  |  3.5   |  4.5   |  5.5   |
| :--: | :----: | :---: | :----: | :----: | :----: |
| L(s) | 1.3087 | 0.754 | 0.2771 | 0.4368 | 1.0644 |

​		由此我们看出，子区域$R_1$的最佳切分点是 s=3.5 。如此继续切分，直到满足设定的条件才停止切分。

4. 满足条件，停止切分，构造树：

​		假如上述例子生成了三个区域后停止切分(即$R_1$找到切分点分成两部分后停止切分，$R_2$满足停止条件没有进行切分)，构造树：

​		切分点是 s =[3.5，6.5]，将整个数据集分成三个区域，我们再计算每个区域的最优估计值$\hat{c}$(即每个子区域的平均值)，汇总成表格：

|     R     | x≤3.5 | 3.5<x≤6.5 | x>6.5 |
| :-------: | :---: | :-------: | :---: |
| $\hat{c}$ | 5.72  |   6.75    | 8.91  |

​		数学描述是：
$$
T={\left\{\begin{array}{l l}{5.72}&{x<3.5}\\ {6.75}&{3.5<x\leq6.5}\\ {8.91}&{x>6.5}\end{array}\right.}
$$
**1.3 代码实现**

```python
from numpy import *
import matplotlib.pyplot as plt
import matplotlib
import re
 
def loadDataSet(fileName):                   # 加载数据集
    dataMat = []
    fr = open(fileName)
    for line in fr.readlines():
        curLine = line.strip().split('\t')
        fltLine = list(map(float,curLine))    # 加上list就是完整功能了，不然只用map会生成内存地址
        dataMat.append(fltLine)
    dataMat = array(dataMat)      # 转化为array数组，便于计算
    return dataMat
 
def binSplitDataSet(dataSet, feature, value):       # 把数据集根据变量(特征)feature按照切分点value分为两类
    mat0 = dataSet[nonzero(dataSet[:,feature] <= value)[0], :]   # 大于成为左子树，实际图形却在右边,为了画图方便我把它改了
    mat1 = dataSet[nonzero(dataSet[:,feature] > value)[0],:]
    return mat0, mat1
 
def regLeaf(dataSet):          # 计算平均值，负责生成叶子节点，叶子节点根据公式不就是求所在子区域的平均值
    return mean(dataSet[:,-1])
 
def regErr(dataSet):           # 方差×个数 = 每个点与估计值的差的平方和，这是根据损失函数公式来的
    return var(dataSet[:,-1]) * shape(dataSet)[0]
def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):
    tols = ops[0]     # 最小下降损失是1
    tolN = ops[1]     # 最小样本数是4
    if len(set(dataSet[:,-1])) == 1:      # 停止切分的条件之一
        return None, leafType(dataSet)
    m,n = shape(dataSet)
    S = errType(dataSet)
    bestS = inf          # 最优切分点的误差
    bestIndex = 0        # 最优切分变量的下标
    bestValue = 0        # 最优切分点
    for featIndex in range(n-1):       # 遍历除最后一列的其他特征，我们的数据集是（x,y），因此这里只能遍历x
        for splitVal in set(dataSet[:,featIndex]):   # splitVal是x可能去到的每一个值作为切分点
            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) # 根据切分点划分为两个子区域
            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):    # 如果两个子区域的叶子节点小于4个，跳过此切分点
                # print("内层判别小于4___________________")
                # print(shape(mat0)[0],shape(mat1)[0])
                continue
            newS = errType(mat0) + errType(mat1)  # 计算两部分的损失函数
            if newS < bestS:              # 如果损失函数小于最优损失函数
                bestIndex = featIndex     # 最优切分变量保存起来
                bestValue = splitVal      # 最优切分点保存起来
                bestS = newS              # 最优损失函数保存起来
    if (S-bestS) < tols:     # 如果切分之前的损失减切分之后的损失小于1,那么就是切分前后损失减小的太慢，停止.它是主要的停止条件
        return None,leafType(dataSet)
    mat0,mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)    # 按照最优切分点分成两个区域
    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] <tolN):      # 如果任何一个子区域的样本数小于4个，停止切分
        # print("外层判断切分样本数小于4******************")       # 个人感觉此判断没用，因为上面选择切分点的时候就把这个情况删除了
        return None, leafType(dataSet)
 
    return bestIndex,bestValue
 
def createTree(dataSet,leafType=regLeaf, errType=regErr, ops=(1,4)):
    feat,val = chooseBestSplit(dataSet, leafType, errType, ops)      # 最优切分点切分
    if feat == None:         # 如果是叶子节点，那么val是那个数据集的平均值，即c
        # print("NOne/执行了",val)
        return val
    # print("没执行",val)
    retTree = {}
    retTree['spInd'] = feat    # 最优切分变量(特征)的下标值
    retTree['spVal'] = val     # 最优切分点s，假如执行到这里，说明找到了最优切分点，将继续切分
    lSet, rSet = binSplitDataSet(dataSet, feat, val)     # 切分成两个子区域，lSet是大于切分点的，相当于我们图形的右边
    retTree['left'] = createTree(lSet, leafType, errType, ops)   # 左子树继续切分
    retTree['right'] = createTree(rSet, leafType, errType, ops)  # 右子树继续切分
    return retTree
 
def drawPicture(dataSet,x_division, y_estimate):     # x_division是切分点，y_estimate是估计值
    matplotlib.rcParams["font.sans-serif"]=["simhei"]   # 显示中文
    matplotlib.rcParams['axes.unicode_minus'] = False
    fig = plt.figure()                 # 创建画布
    ax = fig.add_subplot(111)
    points_x = dataSet[:,0]  # 因为咱们的数据是(x,y)二维图形，x是切分变量，y是估计变量，可参照博客中的具体实例加以理解
    points_y = dataSet[:,1]
 
    x_min = min(points_x)  # 创造切分区域，所以需要x的最小值和最大值，从而构造区域
    x_max = max(points_x)
    y_estimate.append(y_estimate[-1])
 
    ax.step([x_min]+list(x_division)+[x_max],y_estimate,where='post',c='green',linewidth=4,label="最优估计值")  # 画最优估计值
    ax.scatter(points_x, points_y,s=30,c='red',marker='s',label="样本点")   # 画样本点
    ax.legend(loc=4)        # 添加图例
    # ax.grid()               # 添加网格
    ax.set_yticks(y_estimate)   # 设置总坐标刻度
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    plt.show()
 
if __name__ == '__main__':
    myData = loadDataSet('cart-tree.txt')
    retTree = createTree(myData)     # 创建树
    retTree_str = str(retTree)     # 转化为字符串好用正则
    x_division_str = "'spVal': (\d+\.?\d*)"          # 正则提取出切分节点
    y_estimate_str = "'left': (-?\d+\.?\d*)|'right': (-?\d+\.?\d*)"   # 正则表达式取出来叶子节点数据，即最优估计值
    x_division = re.compile(x_division_str).findall(retTree_str)
    y_estimate = re.compile(y_estimate_str).findall(retTree_str)
 
    x_division = sort(list(map(float,x_division)))    # 切分点排序，因为我们画图都是从左往右画，正好对应下面的最优估计值c
    y_estimate = [float(x) for y in y_estimate for x in y if x != '']  # 估计值的顺序不能乱，树中的是什么顺序就是什么顺序
    drawPicture(myData,x_division,y_estimate)    # 画图，画出样本点和最优估计值
```

运行结果：

```python
回归树 = {'spInd': 0, 'spVal': 0.39435, 'left': {'spInd': 0, 'spVal': 0.197834, 'left': -0.023838155555555553, 'right': 1.0289583666666666}, 'right': {'spInd': 0, 'spVal': 0.582002, 'left': 1.980035071428571, 'right': {'spInd': 0, 'spVal': 0.797583, 'left': 2.9836209534883724, 'right': 3.9871632}}}
最优切分点 = [0.197834 0.39435  0.582002 0.797583]
最有估计值 = [-0.023838155555555553, 1.0289583666666666, 1.980035071428571, 2.9836209534883724, 3.9871632]
```

<img src="https://pic.imgdb.cn/item/64d32f6e1ddac507cc602bd4.jpg" style="zoom:67%;" />

<img src="https://pic.imgdb.cn/item/64d336a21ddac507cc7613e1.jpg" style="zoom:67%;" />

**1.4 剪枝算法**

剪枝的过程是：
		从树的最左边最下边开始寻找在同一个子节点上的两个叶子，求他俩平均值，计算误差，其实在同一节点上的两个叶子就是该节点对应的子数据集再次被切分成左右两个区域，左区域的平均值是左叶子，右区域的平均值是右叶子，因此没剪这两个叶子之前的误差就是：**(左区域数据 - 左叶子)的平方  +  (右区域数据 - 右叶子)的平方**合并两个叶子之后的误差就是：**(两个叶子上面的子节点对应的数据集的数据 - 两个叶子的平均值)的平方**
		然后就比较这两个误差的大小，合并后的误差小就进行剪枝，否则就不剪枝。就这样从左往右还得找同一节点的两个叶子进行合并，再进行误差比较，这一过程可以用递归函数来实现。（类似于中序遍历）

<img src="https://pic.imgdb.cn/item/64d332261ddac507cc6872ff.jpg" style="zoom:67%;" />

<img src="https://pic.imgdb.cn/item/64d332321ddac507cc689685.jpg" style="zoom:67%;" />

---

**参考资料：**

[python实现CART回归树](https://blog.csdn.net/weixin_42051109/article/details/88807390?ops_request_misc=&request_id=&biz_id=102&utm_term=chooseBestSplit(dataSet))
